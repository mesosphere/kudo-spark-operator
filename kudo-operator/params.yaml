# Service Accounts and RBAC
operatorServiceAccountName:
  description: "Service Account for Spark Operator"
  default: "spark-operator-service-account"
  displayName: "Operator Service Account"

createOperatorServiceAccount:
  description: "Automatically create Service Account for Spark Operator"
  default: true
  displayName: "Auto Create Operator Service Account"

sparkServiceAccountName:
  description: "Use Service Account for Spark Drivers"
  default: "spark-service-account"
  displayName: "Spark Drivers Service Account"

createSparkServiceAccount:
  description: "Automatically generate Service Account for Spark Drivers"
  default: true
  displayName: "Auto Create Service Account for Spark Drivers"

createRBAC:
  description: "Automatically create RBAC for Spark Operator and Drivers"
  default: true
  displayName: "Auto Create RBAC for Spark Operator and Drivers"

# Docker image
operatorImageName:
  description: "Operator Docker image name"
  default: mesosphere/kudo-spark-operator
  displayName: "Operator Docker image name"

operatorVersion:
  description: "Operator Docker image version (tag)"
  default: latest
  displayName: "Operator Docker image version (tag)"

imagePullPolicy:
  description: "Image pull policy"
  default: Always
  displayName: "Image pull policy"

## Metrics
enableMetrics:
  description: "Enable metrics"
  default: true
  displayName: "Enable metrics"

operatorMetricsPort:
  description: "Spark Operator Metrics port"
  default: 10254
  displayName: "Spark Operator Metrics port"

appMetricsPort:
  description: "Spark Application Metrics port"
  default: 8090
  displayName: "Spark Application Metrics port"

metricsEndpoint:
  description: "Metrics endpoint"
  default: "/metrics"
  displayName: "Metrics endpoint"

metricsPrefix:
  description: "Metrics prefix"
  default: ""
  displayName: "Metrics prefix"

resyncIntervalSeconds:
  description: "Informer resync interval in seconds"
  default: 30
  displayName: "Informer resync interval in seconds"

## Miscellaneous
sparkJobNamespace:
  description: "Namespace for Spark Applications. Defaults to Operator namespace"
  default: ""
  displayName: "Namespace for Spark Applications"

enableWebhook:
  description: "Enable webhook"
  default: true
  displayName: "Enable webhook"

webhookPort:
  description: "Webhook port"
  default: 8080
  displayName: "Webhook port"

controllerThreads:
  description: "Controller threads"
  default: 10
  displayName: "Controller threads"

ingressUrlFormat:
  description: "Ingress URL format"
  default: ""
  displayName: "Ingress URL format"

enableHistoryServer:
  description: "Start Spark History Server"
  default: false
  displayName: "Enable Spark History Server"

historyServerFsLogDirectory:
  description: "Spark EventLog Directory from which to load events for prior Spark job runs (e.g., hdfs://hdfs/ or s3a://path/to/bucket). Note: You'd typically want to set this to point to distributed/HA storage"
  default: ""
  displayName: "Spark History Server Event Log Directory"

historyServerCleanerEnabled:
  description: "Specifies whether the Spark History Server should periodically clean up event logs from storage."
  default: "false"
  displayName: "Spark History Server Cleaner Enabled"

historyServerCleanerInterval:
  description: "Frequency the Spark History Server checks for files to delete."
  default: "1d"
  displayName: "Spark History Server Cleaner Interval"

historyServerCleanerMaxAge:
  description: "History files older than this will be deleted."
  default: "7d"
  displayName: "Spark History Server Cleaner Max Age"

historyServerOpts:
  description: "Extra options to pass to the Spark History Server"
  default: ""
  displayName: "Spark History Server Options"

historyServerPVCName:
  description: "External Persistent Volume Claim Name used for Spark event logs storage by History Server"
  default: ""
  displayName: "Spark History Server Persistent Volume Claim Name"

## Node labels for pod assignment
## Ref: https://kubernetes.io/docs/user-guide/node-selection/
## TODO: should be a map
nodeSelector:
  default: ""

## Resources for the sparkoperator deployment
## Ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
## TODO: should be a map
resources:
  default: ""

## TODO: should be a map
podAnnotations:
  default: ""


## Output verbosity and debugging level
## Ref: https://kubernetes.io/docs/reference/kubectl/cheatsheet/#kubectl-output-verbosity-and-debugging
logLevel:
  description: "Log Level"
  default: 2
  displayName: "Log Level"

## Whether to enable batch scheduler for pod scheduling,
## if enabled, end user can specify batch scheduler name in spark application.
enableBatchScheduler:
  description: "Enable Batch Scheduler"
  default: false
  displayName: "Enable Batch Scheduler"

installCrds:
  description: "Install sparkapplications.sparkoperator.k8s.io and scheduledsparkapplications.sparkoperator.k8s.io"
  default: false
  displayName: "Install Spark Operator CRDs"
